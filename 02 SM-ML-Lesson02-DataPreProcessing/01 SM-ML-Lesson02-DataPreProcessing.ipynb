{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5547671c-f7fa-4d4e-b517-fd9650040848",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e19e0-16da-4734-a53e-83d8480b53fe",
   "metadata": {},
   "source": [
    "Data preprocessing in machine learning (ML) refers to the set of techniques used to clean, transform, and prepare raw data for model training. It's a critical step because the quality of data directly impacts the performance and accuracy of ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee87fe-909e-46af-8105-3974071c347e",
   "metadata": {},
   "source": [
    "### Overview of Data Preprocessing\n",
    "\n",
    "1.  **Importance of Data Preprocessing**:\n",
    "\n",
    "    -   **Data Quality**: Raw data often contains errors, missing values, outliers, and inconsistencies that can affect model performance.\n",
    "    -   **Model Performance**: Proper preprocessing ensures data is in a suitable format, improving the accuracy and reliability of ML models.\n",
    "    -   **Algorithm Compatibility**: ML algorithms often have specific requirements regarding data types, scales, and distributions.\n",
    "2.  **Steps in Data Preprocessing**:\n",
    "\n",
    "    -   **Data Cleaning**: Handling missing data, dealing with outliers, and resolving inconsistencies.\n",
    "    -   **Data Transformation**: Scaling, normalization, encoding categorical variables, and feature engineering.\n",
    "    -   **Data Reduction**: Dimensionality reduction to reduce computational complexity.\n",
    "\n",
    "### Detailed Techniques and Methods\n",
    "\n",
    "#### 1\\. **Data Cleaning**\n",
    "\n",
    "-   **Handling Missing Data**:\n",
    "\n",
    "    -   **Imputation**: Replace missing values with statistical measures (mean, median, mode) or more advanced techniques like predictive modeling (e.g., KNN imputation).\n",
    "    -   **Deletion**: Remove rows or columns with missing values if they are insignificant or too many.\n",
    "-   **Handling Outliers**:\n",
    "\n",
    "    -   Identify outliers using statistical methods (e.g., Z-score, IQR) and decide whether to remove, transform, or treat them as special cases based on domain knowledge.\n",
    "-   **Handling Duplicates**:\n",
    "\n",
    "    -   Identify and remove duplicate records to avoid biasing the model.\n",
    "\n",
    "#### 2\\. **Data Transformation**\n",
    "\n",
    "-   **Normalization**:\n",
    "\n",
    "    -   Scale numerical features to a standard range (e.g., 0 to 1) to ensure all features contribute equally to the model.\n",
    "    -   Techniques include Min-Max scaling and Z-score normalization.\n",
    "-   **Standardization**:\n",
    "\n",
    "    -   Transform data to have a mean of 0 and a standard deviation of 1. Useful for algorithms that assume normally distributed data.\n",
    "-   **Encoding Categorical Variables**:\n",
    "\n",
    "    -   **One-Hot Encoding**: Convert categorical variables into binary vectors (0s and 1s) to make them suitable for ML algorithms.\n",
    "    -   **Label Encoding**: Convert categorical variables into numeric labels.\n",
    "-   **Handling Skewed Data**:\n",
    "\n",
    "    -   Apply transformations like logarithmic or power transformations to make skewed data distributions more symmetric and easier for models to interpret.\n",
    "\n",
    "#### 3\\. **Feature Engineering**\n",
    "\n",
    "-   **Creating New Features**:\n",
    "\n",
    "    -   Derive new features from existing ones that may improve model performance. For example, extracting date features (day of the week, month) from timestamps.\n",
    "-   **Feature Scaling**:\n",
    "\n",
    "    -   Ensure all features are on the same scale to prevent some features from dominating the learning process.\n",
    "\n",
    "#### 4\\. **Data Integration**\n",
    "\n",
    "-   **Merge and Concatenate Data**:\n",
    "    -   Combine data from different sources into a single dataset for analysis and model training.\n",
    "\n",
    "#### 5\\. **Data Reduction**\n",
    "\n",
    "-   **Dimensionality Reduction**:\n",
    "    -   Techniques like Principal Component Analysis (PCA) or t-Distributed Stochastic Neighbor Embedding (t-SNE) reduce the number of variables in a dataset while preserving important information.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "-   **Exploratory Data Analysis (EDA)**:\n",
    "\n",
    "    -   Understand the data distribution, relationships between variables, and identify patterns and anomalies before preprocessing.\n",
    "-   **Documentation and Reproducibility**:\n",
    "\n",
    "    -   Document all preprocessing steps and transformations applied to ensure reproducibility and transparency in the model development process.\n",
    "-   **Handling Imbalanced Data**:\n",
    "\n",
    "    -   Techniques like oversampling (e.g., SMOTE), undersampling, or using class weights in algorithms can address class imbalance issues in datasets.\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "-   **Python Libraries**: `pandas`, `NumPy`, `scikit-learn` for data manipulation and preprocessing.\n",
    "-   **Visualization Tools**: `matplotlib`, `seaborn` for EDA.\n",
    "\n",
    "### Considerations\n",
    "\n",
    "-   **Domain Knowledge**: Understanding the domain-specific characteristics of data helps in making informed preprocessing decisions.\n",
    "\n",
    "-   **Iterative Process**: Data preprocessing is often iterative, where insights gained during model training may lead to additional preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fd315-fc35-4097-a195-f83c3bfbd527",
   "metadata": {},
   "source": [
    "Let's walk through the data preprocessing steps with some live examples using a sample dataset. We'll use Python and some popular libraries (pandas, NumPy, scikit-learn) to demonstrate each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a3cb8-9970-4231-83d7-b1782f8089c4",
   "metadata": {},
   "source": [
    "### Example Dataset\n",
    "\n",
    "Let's consider a dataset for predicting house prices. The dataset has the following columns:\n",
    "\n",
    "-   `SquareFeet`\n",
    "-   `Bedrooms`\n",
    "-   `Bathrooms`\n",
    "-   `Neighborhood`\n",
    "-   `YearBuilt`\n",
    "-  Â `Price`\n",
    "\n",
    "1. **Loading the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f33cb3c-8864-4fbc-b18f-32b037894311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SquareFeet  Bedrooms  Bathrooms Neighborhood  YearBuilt          Price\n",
      "0        2126         4          1        Rural       1969  215355.283618\n",
      "1        2459         3          2        Rural       1980  195014.221626\n",
      "2        1860         2          1       Suburb       1970  306891.012076\n",
      "3        2294         2          1        Urban       1996  206786.787153\n",
      "4        2130         5          2       Suburb       2001  272436.239065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('../Datasets/housing_price_dataset.csv')\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f265364-4f6f-4dcc-ae15-6d19e442ef54",
   "metadata": {},
   "source": [
    "### 2\\. **Handling Missing Data**\n",
    "\n",
    "Let's fill missing values for `SquareFeet` and `Bathrooms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a34b096-b3f9-476e-926e-5a198939f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean of the column\n",
    "data['SquareFeet'].fillna(data['SquareFeet'].mean(), inplace=True)\n",
    "data['Bathrooms'].fillna(data['Bathrooms'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64543f9-a29a-42e9-a489-ce8183d29f79",
   "metadata": {},
   "source": [
    "### 3\\. **Handling Outliers**\n",
    "\n",
    "Assuming `Price` has some outliers, we can use the Z-score method to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad0a2f7b-8651-4dd3-bd9c-25b7fa919e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Remove outliers in 'Price' using Z-score\n",
    "data = data[(abs(stats.zscore(data['Price'])) < 3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc093a-bf7f-4570-ba00-7878f829a9a5",
   "metadata": {},
   "source": [
    "### 4\\. **Encoding Categorical Variables**\n",
    "\n",
    "Convert the `Neighborhood` column to numeric using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b7e60c-ccb1-47ed-be54-43e2354fa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for 'Neighborhood'\n",
    "data = pd.get_dummies(data, columns=['Neighborhood'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9da69-3819-45fd-930d-4e4a165a1ccc",
   "metadata": {},
   "source": [
    "### 5\\. **Normalization/Standardization**\n",
    "\n",
    "Normalize `SquareFeet` and `Price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6e08e0-f434-4035-916b-34d498624f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Normalizing 'SquareFeet'\n",
    "scaler = MinMaxScaler()\n",
    "data[['SquareFeet']] = scaler.fit_transform(data[['SquareFeet']])\n",
    "\n",
    "# Standardizing 'Price'\n",
    "scaler = StandardScaler()\n",
    "data[['Price']] = scaler.fit_transform(data[['Price']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dafa4c-bda4-40b2-9520-7da54ee1b82f",
   "metadata": {},
   "source": [
    "### 6\\. **Feature Engineering**\n",
    "\n",
    "Create new features such as the age of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c0a88cb-abe6-4bc5-b95b-dcb38d060c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Create a new feature 'HouseAge'\n",
    "current_year = datetime.datetime.now().year\n",
    "data['HouseAge'] = current_year - data['YearBuilt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86030af1-f86c-4249-b01d-4582d78d373c",
   "metadata": {},
   "source": [
    "### 7\\. **Final Dataset**\n",
    "\n",
    "Print the first few rows of the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb8d428-cc69-488f-ae95-52574b165357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SquareFeet  Bedrooms  Bathrooms  YearBuilt     Price  Neighborhood_Suburb  \\\n",
      "0    0.563282         4          1       1969 -0.124972                False   \n",
      "1    0.729865         3          2       1980 -0.392963                False   \n",
      "2    0.430215         2          1       1970  1.080998                 True   \n",
      "3    0.647324         2          1       1996 -0.237861                False   \n",
      "4    0.565283         5          2       2001  0.627062                 True   \n",
      "\n",
      "   Neighborhood_Urban  HouseAge  \n",
      "0               False        55  \n",
      "1               False        44  \n",
      "2               False        54  \n",
      "3                True        28  \n",
      "4               False        23  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb30f85-c97e-4ea0-90d2-31f0920c3396",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "\n",
    "Here's a complete example script that includes all the steps above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "769b7eec-eca4-4df7-a45c-1936edb9fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SquareFeet  Bedrooms  Bathrooms  YearBuilt     Price  Neighborhood_Suburb  \\\n",
      "0    0.563282         4          1       1969 -0.124972                False   \n",
      "1    0.729865         3          2       1980 -0.392963                False   \n",
      "2    0.430215         2          1       1970  1.080998                 True   \n",
      "3    0.647324         2          1       1996 -0.237861                False   \n",
      "4    0.565283         5          2       2001  0.627062                 True   \n",
      "\n",
      "   Neighborhood_Urban  HouseAge  \n",
      "0               False        55  \n",
      "1               False        44  \n",
      "2               False        54  \n",
      "3                True        28  \n",
      "4               False        23  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import datetime\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('../Datasets/housing_price_dataset.csv')\n",
    "\n",
    "# Fill missing values\n",
    "data['SquareFeet'].fillna(data['SquareFeet'].mean(), inplace=True)\n",
    "data['Bathrooms'].fillna(data['Bathrooms'].mean(), inplace=True)\n",
    "\n",
    "# Remove outliers in 'Price'\n",
    "data = data[(abs(stats.zscore(data['Price'])) < 3)]\n",
    "\n",
    "# One-Hot Encoding for 'Neighborhood'\n",
    "data = pd.get_dummies(data, columns=['Neighborhood'], drop_first=True)\n",
    "\n",
    "# Normalize and standardize\n",
    "scaler = MinMaxScaler()\n",
    "data[['SquareFeet']] = scaler.fit_transform(data[['SquareFeet']])\n",
    "scaler = StandardScaler()\n",
    "data[['Price']] = scaler.fit_transform(data[['Price']])\n",
    "\n",
    "# Feature engineering\n",
    "current_year = datetime.datetime.now().year\n",
    "data['HouseAge'] = current_year - data['YearBuilt']\n",
    "\n",
    "# Final processed data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c63e09-5405-44d7-9bc7-a4ed5d0a31d6",
   "metadata": {},
   "source": [
    "### Explanation of Each Step\n",
    "\n",
    "1.  **Loading the Dataset**: Importing the dataset into a pandas DataFrame for easier manipulation and analysis.\n",
    "2.  **Handling Missing Data**: Filling missing values with the mean of their respective columns to avoid data loss.\n",
    "3.  **Handling Outliers**: Removing outliers in the `Price` column using the Z-score method to improve model accuracy.\n",
    "4.  **Encoding Categorical Variables**: Converting the categorical `Neighborhood` column into numerical format using one-hot encoding.\n",
    "5.  **Normalization/Standardization**: Scaling the `SquareFeet` column to the range [0, 1] using Min-Max scaling and standardizing the `Price` column to have a mean of 0 and a standard deviation of 1.\n",
    "6.  **Feature Engineering**: Creating a new feature `HouseAge` to represent the age of the house, which might be a significant predictor of house price.\n",
    "7.  **Final Dataset**: Displaying the processed data to verify that all preprocessing steps have been applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef1c187-f195-4947-9e39-863245a1f238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
